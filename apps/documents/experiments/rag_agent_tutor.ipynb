{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6595a434",
   "metadata": {},
   "source": [
    "### Setup env and mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad242e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/02 18:08:28 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/01/02 18:08:28 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/01/02 18:08:28 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/02 18:08:28 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/01/02 18:08:28 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/01/02 18:08:28 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Optional: Set an experiment to organize your traces\n",
    "mlflow.set_experiment(\"RAG Agent Tutor\")\n",
    "\n",
    "# Enable tracing\n",
    "mlflow.langchain.autolog()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8738f99c",
   "metadata": {},
   "source": [
    "### Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1acfd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Justin Bieber was born on **March 1, 1994**. The Super Bowl played in the calendar year 1994 was **Super Bowl XXVIII**.\n",
      "\n",
      "Here are the details:\n",
      "*   **Champion:** **Dallas Cowboys**\n",
      "*   **Opponent:** Buffalo Bills\n",
      "*   **Score:** 30–13\n",
      "*   **Date:** January 30, 1994\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"xiaomi/mimo-v2-flash:free\",\n",
    "    model_provider=\"openai\",\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=getenv(\"OPENROUTER_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "response = model.invoke(\"What NFL team won the Super Bowl in the year Justin Bieber was born?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe3b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "from pydantic import SecretStr\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_localai import LocalAIEmbeddings\n",
    "\n",
    "embeddings = LocalAIEmbeddings(\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\", \n",
    "    model=\"openai/text-embedding-3-small\", \n",
    "    openai_api_key=getenv(\"OPENROUTER_API_KEY\"), \n",
    "    # headers={\"Authorization\": f\"Bearer {getenv('OPENROUTER_API_KEY')}\"},\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3e7f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n",
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "from bs4.filter import SoupStrainer\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")\n",
    "\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "089bd79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a3488f1b-8ae6-45db-97da-59817aaa599f', '20434d42-9991-4e60-9621-6133f5599316', '5c4718dd-0bc3-40ad-8fcb-8d9d2432aaf9']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd827a88",
   "metadata": {},
   "source": [
    "### RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dea9d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1c3cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a297e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the standard method for Task Decomposition?\n",
      "\n",
      "Once you get the answer, look up common extensions of that method.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "First, I need to understand what the user is asking. The question is: \"What is the standard method for Task Decomposition?\" and then \"Once you get the answer, look up common extensions of that method.\"\n",
      "So, Task Decomposition is a term I've heard before, but I'm not entirely sure what it specifically refers to in different contexts. It could be related to project management, computer science, or perhaps even cognitive psychology. I think the most common context is in project management or software development, where tasks are broken down into smaller, more manageable parts.\n",
      "But to be precise, I should probably look this up to make sure I have the correct definition and standard method.\n",
      "Let me think about how to approach this. Since the user mentioned a blog post, I should use the retrieve_context tool to find relevant information about Task Decomposition.\n",
      "Wait, actually, the instructions say: \"You have access to a tool that retrieves context from a blog post.\" So, I need to use the retrieve_context tool to get information from a specific blog post. But the user didn't specify which blog post, so maybe the tool is designed to retrieve from a general knowledge base or a specific set of blog posts related to the topic.\n",
      "Looking back at the tool description: \"Retrieve information to help answer a query.\" It doesn't specify the source, so perhaps it's a general retrieval tool.\n",
      "In any case, I should start by using the retrieve_context tool with a query like \"standard method for Task Decomposition.\"\n",
      "Let me formulate that.\n",
      "First function call:\n",
      "Tool Calls:\n",
      "  retrieve_context (call_bce47af857d1418aa070ec84)\n",
      " Call ID: call_bce47af857d1418aa070ec84\n",
      "  Args:\n",
      "    query: standard method for Task Decomposition\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Alright, from the search results, it seems that the standard method for Task Decomposition is related to Chain of Thought (CoT) prompting, especially in the context of language models. The first result mentions that task decomposition can be done by LLM with simple prompting, using task-specific instructions, or with human inputs. It also mentions LLM+P, which involves external classical planners, but that seems more specific.\n",
      "The second result directly talks about Chain of Thought (CoT) as a standard prompting technique for enhancing model performance on complex tasks by decomposing them into smaller steps.\n",
      "So, based on this, I think the standard method for Task Decomposition, at least in the context of AI and language models, is the Chain of Thought (CoT) method.\n",
      "But wait, is Task Decomposition only relevant in AI and language models? The user didn't specify the context, so maybe it's broader. However, given that the tool retrieved information from a blog post about AI agents, it's likely that the context is AI-related.\n",
      "Let me check if there are other contexts for Task Decomposition. For example, in project management, Task Decomposition might refer to breaking down a project into smaller tasks, which is a fundamental practice. But I think the user is probably referring to the AI context, especially since the second part of the question asks for common extensions of the method, which sounds technical.\n",
      "To confirm, maybe I can do another search for \"Task Decomposition\" in a broader sense.\n",
      "Wait, actually, in the first function result, it mentioned \"Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique...\" So, it's explicitly stating that CoT is a standard technique for task decomposition in the context of AI.\n",
      "Moreover, the blog post is about AI agents, so it's safe to assume that's the context.\n",
      "Therefore, the standard method for Task Decomposition is the Chain of Thought (CoT) method.\n",
      "Now, the next part is to look up common extensions of that method.\n",
      "From the search results, I see that Tree of Thoughts (ToT) is mentioned as an extension of CoT. It says: \"Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step.\"\n",
      "So, ToT is an extension of CoT.\n",
      "Is there anything else? The first result also mentioned LLM+P, which involves using external planners, but that seems more like an alternative method rather than an extension of CoT.\n",
      "Wait, let's read carefully.\n",
      "In the first result, it says: \"Task decomposition can be done (1) by LLM with simple prompting like 'Steps for XYZ.\\n1.', 'What are the subgoals for achieving XYZ?', (2) by using task-specific instructions; e.g. 'Write a story outline.' for writing a novel, or (3) with human inputs. Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner...\"\n",
      "So, the first three methods are ways to do task decomposition, and LLM+P is another approach.\n",
      "But the standard method seems to be the CoT, as it's mentioned as a standard prompting technique.\n",
      "Wait, actually, in the second result, it says: \"Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique...\"\n",
      "So, yes, CoT is the standard.\n",
      "Now, for extensions, ToT is one.\n",
      "Are there others?\n",
      "Maybe I can search for \"extensions of Chain of Thought prompting\" or something similar.\n",
      "Let me try that.\n",
      "Second function call:\n",
      "Tool Calls:\n",
      "  retrieve_context (call_575924d3292f49b0ba38ec3d)\n",
      " Call ID: call_575924d3292f49b0ba38ec3d\n",
      "  Args:\n",
      "    query: extensions of Chain of Thought prompting\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 25955}\n",
      "Content: Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\n",
      "\n",
      "\n",
      "Planning & Reacting: translate the reflections and the environment information into actions\n",
      "\n",
      "Planning is essentially in order to optimize believability at the moment vs in time.\n",
      "Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\n",
      "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
      "Environment information is present in a tree structure.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The generative agent architecture. (Image source: Park et al. 2023)\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hmm, this search didn't give me new information about extensions of CoT. It mostly repeated the same information about CoT and ToT.\n",
      "Maybe I need to be more specific or try a different query.\n",
      "Alternatively, perhaps I can look for papers or articles that discuss advancements or extensions beyond CoT.\n",
      "Wait, another idea: since the blog post mentioned ToT as an extension, maybe there are other extensions like Graph of Thoughts or something similar.\n",
      "Let me try searching for \"Tree of Thoughts\" or \"Graph of Thoughts.\"\n",
      "But first, perhaps I can check if there are other extensions mentioned in the same blog post.\n",
      "Looking at the function result, it only mentions ToT as an extension.\n",
      "Wait, actually, in the first search result, there was mention of LLM+P, but that's a different approach.\n",
      "Alternatively, maybe I can search for \"advancements in Chain of Thought prompting\" or \"improvements to CoT.\"\n",
      "Let me try that.\n",
      "Third function call:\n",
      "Tool Calls:\n",
      "  retrieve_context (call_dbf142a8b0624800b0d33ab6)\n",
      " Call ID: call_dbf142a8b0624800b0d33ab6\n",
      "  Args:\n",
      "    query: advancements in Chain of Thought prompting\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 40518}\n",
      "Content: Or\n",
      "@article{weng2023agent,\n",
      "  title   = \"LLM-powered Autonomous Agents\",\n",
      "  author  = \"Weng, Lilian\",\n",
      "  journal = \"lilianweng.github.io\",\n",
      "  year    = \"2023\",\n",
      "  month   = \"Jun\",\n",
      "  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
      "}\n",
      "References#\n",
      "[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\n",
      "[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\n",
      "[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\n",
      "“ arXiv preprint arXiv:2302.02676 (2023).\n",
      "[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\n",
      "[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\n",
      "[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\n",
      "[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Again, the same information. It seems like the blog post I'm retrieving from is the main source, and it primarily discusses CoT and ToT.\n",
      "Perhaps I need to look elsewhere for more extensions.\n",
      "Wait, maybe I can try to search for \"Chain of Thought extensions\" or something similar, but specify a different source if possible.\n",
      "Alternatively, since the tool retrieves from a blog post, maybe I can assume that the blog post has more information, but from the excerpts, it only mentions ToT.\n",
      "Wait a minute, in the references, there is a paper titled \"Chain of Hindsight Aligns Language Models with Feedback\" by Liu et al. 2023, and also \"ReAct: Synergizing reasoning and acting in language models\" by Yao et al. 2023.\n",
      "Maybe these are related to extensions of CoT.\n",
      "Let me check what ReAct is.\n",
      "From the name, \"ReAct\" sounds like it combines reasoning and acting, which might be an extension of CoT.\n",
      "Similarly, \"Chain of Hindsight\" might be another extension.\n",
      "So, perhaps I can consider these as common extensions.\n",
      "But to be sure, I need to understand what they are.\n",
      "Maybe I can do a search for \"ReAct prompting\" or \"Chain of Hindsight.\"\n",
      "Let's try searching for \"ReAct prompting.\"\n",
      "Fourth function call:\n",
      "Tool Calls:\n",
      "  retrieve_context (call_2c52e91b806f4b01bf5c7aa3)\n",
      " Call ID: call_2c52e91b806f4b01bf5c7aa3\n",
      "  Args:\n",
      "    query: ReAct prompting\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 4312}\n",
      "Content: Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\n",
      "\n",
      "In both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\n",
      "Reflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\n",
      "\n",
      "\n",
      "Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 3549}\n",
      "Content: Self-Reflection#\n",
      "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\n",
      "ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\n",
      "The ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\n",
      "Thought: ...\n",
      "Action: ...\n",
      "Observation: ...\n",
      "... (Repeated many times)\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "From this, it seems that ReAct is a method that integrates reasoning and acting, which might be an extension or a variant of CoT.\n",
      "Similarly, Reflexion is mentioned, which is a framework for self-reflection to improve reasoning skills.\n",
      "So, possible extensions of CoT include:\n",
      "1. Tree of Thoughts (ToT)\n",
      "2. ReAct\n",
      "3. Reflexion\n",
      "Wait, but is Reflexion directly an extension of CoT? It seems more related to self-reflection and improving reasoning over time.\n",
      "Similarly, ReAct combines reasoning with actions, which might be a different approach.\n",
      "Let me see if there are more direct extensions.\n",
      "Alternatively, perhaps I can look for \"advanced CoT methods\" or something like that.\n",
      "Wait, another idea: maybe there is something called \"Graph of Thoughts\" or \"Program of Thoughts.\"\n",
      "Let me try searching for \"Graph of Thoughts.\"\n",
      "Fifth function call:\n",
      "Tool Calls:\n",
      "  retrieve_context (call_337b2a8bf14740468a467850)\n",
      " Call ID: call_337b2a8bf14740468a467850\n",
      "  Args:\n",
      "    query: Graph of Thoughts\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 5209}\n",
      "Content: Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "\n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\n",
      "\n",
      "\n",
      "Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hmm, still not finding \"Graph of Thoughts.\" Maybe it's not a standard term, or perhaps it's under a different name.\n",
      "Alternatively, perhaps I can look for other papers or methods that build upon CoT.\n",
      "Wait, in the references, there is \"LLM+P: Empowering Large Language Models with Optimal Planning Proficiency,\" which might be related.\n",
      "But from the earlier description, LLM+P seems to use external planners, which is different from CoT.\n",
      "Another reference is \"Chain of Hindsight Aligns Language Models with Feedback,\" which might be related to using feedback to improve the chain.\n",
      "Let me try to find out more about Chain of Hindsight.\n",
      "Sixth function call:\n",
      "Tool Calls:\n",
      "  retrieve_context (call_62fe7bcbe6ce45e9b2bb304e)\n",
      " Call ID: call_62fe7bcbe6ce45e9b2bb304e\n",
      "  Args:\n",
      "    query: Chain of Hindsight\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 6069}\n",
      "Content: Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 5209}\n",
      "Content: Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\n",
      "\n",
      "The heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\n",
      "Self-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\n",
      "\n",
      "\n",
      "Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "From this, Chain of Hindsight seems to be a method where the model is fine-tuned with a sequence of past outputs and feedback to improve its performance. It's more about learning from past mistakes and feedback rather than directly decomposing tasks.\n",
      "So, while it's related to improving reasoning, it might not be a direct extension of CoT for task decomposition.\n",
      "Similarly, ReAct integrates reasoning with actions, which is useful for interactive tasks.\n",
      "Perhaps for task decomposition specifically, ToT is the primary extension.\n",
      "Wait, let's think differently. Maybe there are other methods that build upon CoT for task decomposition.\n",
      "For example, is there something like \"Multi-step CoT\" or \"Recursive CoT\"?\n",
      "Alternatively, perhaps \"Self-Consistency\" is related, but I think that's a decoding strategy.\n",
      "Wait, another idea: in the original CoT paper, there might be mentions of variations or extensions.\n",
      "But since I don't have direct access to papers, I need to rely on the blog post or other sources.\n",
      "Wait, perhaps I can try to search for \"task decomposition methods in AI\" to see if there are other standard methods besides CoT.\n",
      "Seventh function call:\n",
      "Tool Calls:\n",
      "  retrieve_context (call_e3ad722843f048f0872ae62e)\n",
      " Call ID: call_e3ad722843f048f0872ae62e\n",
      "  Args:\n",
      "    query: task decomposition methods in AI\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "This search again points back to the same blog post, which mentions CoT, ToT, and LLM+P.\n",
      "Perhaps in the context of this blog post, the standard method is CoT, and the common extension is ToT.\n",
      "Alternatively, maybe I can consider LLM+P as an extension, but it's presented as a distinct approach.\n",
      "Wait, let's see the user's question again: \"What is the standard method for Task Decomposition? Once you get the answer, look up common extensions of that method.\"\n",
      "So, first, identify the standard method, which from the information is CoT.\n",
      "Then, find common extensions of CoT.\n",
      "From the blog post, ToT is explicitly mentioned as an extension.\n",
      "Additionally, ReAct and Reflexion are mentioned, but they might be more related to reasoning and acting rather than directly extending CoT for task decomposition.\n",
      "Wait, perhaps I can check if there is a specific section on extensions.\n",
      "Alternatively, maybe I can try to find out if there are other well-known extensions.\n",
      "Let me try to think if there are any other methods I recall.\n",
      "For example, there's \"Least-to-Most Prompting,\" which is another technique for complex reasoning.\n",
      "Wait, let's search for that.\n",
      "Eighth function call:\n",
      "Tool Calls:\n",
      "  retrieve_context (call_55b1bbecfd7f40589867108e)\n",
      " Call ID: call_55b1bbecfd7f40589867108e\n",
      "  Args:\n",
      "    query: Least-to-Most Prompting\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 25955}\n",
      "Content: Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\n",
      "\n",
      "\n",
      "Planning & Reacting: translate the reflections and the environment information into actions\n",
      "\n",
      "Planning is essentially in order to optimize believability at the moment vs in time.\n",
      "Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\n",
      "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
      "Environment information is present in a tree structure.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The generative agent architecture. (Image source: Park et al. 2023)\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Again, the same information. It seems like the blog post is the primary source, and it doesn't mention Least-to-Most Prompting.\n",
      "Perhaps Least-to-Most Prompting is another method, but not necessarily an extension of CoT.\n",
      "Wait, actually, upon thinking, Least-to-Most Prompting is a technique where you first prompt the model to break down the problem into subproblems, then solve each subproblem sequentially.\n",
      "That sounds similar to task decomposition, but it's a different prompting strategy.\n",
      "However, since the user asked for extensions of the standard method, and the standard method is CoT, I think ToT is the main extension mentioned.\n",
      "Alternatively, maybe I can consider that CoT itself has variations, like zero-shot CoT or few-shot CoT, but those are more about how to prompt rather than extensions.\n",
      "Wait, the blog post mentions that CoT can be done with simple prompting or with task-specific instructions, but ToT is explicitly an extension.\n",
      "I think I've gathered enough information.\n",
      "To summarize:\n",
      "- Standard method for Task Decomposition: Chain of Thought (CoT)\n",
      "- Common extensions: Tree of Thoughts (ToT)\n",
      "Additionally, there are other related methods like ReAct and Reflexion, but ToT is directly mentioned as an extension of CoT.\n",
      "So, I think the answer should be:\n",
      "The standard method for Task Decomposition is the Chain of Thought (CoT) technique. Common extensions of CoT include the Tree of Thoughts (ToT) method, which explores multiple reasoning possibilities at each step.\n",
      "But to be thorough, perhaps I can confirm if there are other extensions.\n",
      "Wait, one more idea: maybe \"Program of Thoughts\" (PoT) is another extension.\n",
      "Let me check.\n",
      "Ninth function call:\n",
      "Tool Calls:\n",
      "  retrieve_context (call_a2af0718695b4fad829577b3)\n",
      " Call ID: call_a2af0718695b4fad829577b3\n",
      "  Args:\n",
      "    query: Program of Thoughts\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 3549}\n",
      "Content: Self-Reflection#\n",
      "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\n",
      "ReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\n",
      "The ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\n",
      "Thought: ...\n",
      "Action: ...\n",
      "Observation: ...\n",
      "... (Repeated many times)\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "No mention of Program of Thoughts in this source. Maybe it's not covered.\n",
      "Alternatively, perhaps it's a different method.\n",
      "Given that, I think it's safe to conclude that the primary extension mentioned is Tree of Thoughts.\n",
      "Therefore, I can proceed to formulate the final answer.\n",
      "But wait, the user said \"Once you get the answer, look up common extensions of that method.\" So, I need to make sure I have at least one common extension.\n",
      "From the information, ToT is clearly an extension.\n",
      "Additionally, perhaps I can mention that there are other methods like ReAct and Reflexion that build upon the idea of reasoning in steps, but they might not be direct extensions for task decomposition.\n",
      "Alternatively, to be precise, I'll stick with ToT as the common extension.\n",
      "So, final answer:The standard method for Task Decomposition is the Chain of Thought (CoT) technique, which involves instructing the model to think step by step to decompose complex tasks into smaller, manageable steps. A common extension of CoT is the Tree of Thoughts (ToT) method, which enhances the reasoning process by exploring multiple reasoning possibilities at each step and creating a tree structure for more comprehensive problem-solving.- [Lilian Weng's Blog on LLM-powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)\n",
      "Wait, actually, since the tool provided the source, I should cite that.\n",
      "From the function results, the source is 'https://lilianweng.github.io/posts/2023-06-23-agent/'\n",
      "So, I can cite that.\n",
      "Also, to be thorough, perhaps I can mention that CoT was introduced by Wei et al. in 2022, and ToT by Yao et al. in 2023, as per the references in the blog post.\n",
      "But since the user didn't ask for citations, maybe it's not necessary.\n",
      "However, following the example in the workflow, it's good to include citations.\n",
      "So, I'll include it.The standard method for Task Decomposition is the Chain of Thought (CoT) technique, which involves instructing the model to think step by step to decompose complex tasks into smaller, manageable steps. A common extension of CoT is the Tree of Thoughts (ToT) method, which enhances the reasoning process by exploring multiple reasoning possibilities at each step and creating a tree structure for more comprehensive problem-solving.- [Lilian Weng's Blog on LLM-powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)\n",
      "The standard method for task decomposition is Chain of Thought (CoT), which breaks complex tasks into manageable steps; its most notable extension is Tree of Thoughts (ToT), which enhances problem-solving by exploring multiple reasoning paths, though other methods like ReAct and Reflexion are also being explored for more advanced AI reasoning.\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab380591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec31d8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is task decomposition?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Based on the provided context, **task decomposition** is the process of breaking down a large, complicated task into smaller, more manageable sub-tasks or steps.\n",
      "\n",
      "Here are the three main ways task decomposition can be done:\n",
      "\n",
      "1.  **By LLM with Simple Prompting:** This involves instructing the model to break down the task itself, using prompts like \"Steps for XYZ. 1.\" or \"What are the subgoals for achieving XYZ?\"\n",
      "2.  **Using Task-Specific Instructions:** This utilizes specific guidelines for a particular domain, such as \"Write a story outline\" as a sub-task for writing a novel.\n",
      "3.  **With Human Inputs:** Relying on human guidance to define the steps.\n",
      "\n",
      "Additionally, the text highlights specific techniques for handling complex tasks:\n",
      "\n",
      "*   **Chain of Thought (CoT):** A standard prompting technique where the model is instructed to \"think step by step\" to decompose hard tasks into simpler steps, utilizing more computation time to solve the problem.\n",
      "*   **Tree of Thoughts:** This extends CoT by exploring multiple reasoning possibilities at each step. It creates a tree structure by generating multiple thoughts per step and using search algorithms (like BFS or DFS) to evaluate different paths.\n",
      "*   **LLM+P:** An approach that outsources the planning to an external classical planner using the Planning Domain Definition Language (PDDL). The LLM translates the problem into PDDL, a planner generates a plan, and the LLM translates that plan back into natural language.\n",
      "\n",
      "In systems like HuggingGPT, task decomposition involves parsing user input into a structured list of tasks, defining their type, ID, dependencies, and arguments.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is task decomposition?\"\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f1060b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "documents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
